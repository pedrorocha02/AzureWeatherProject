{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61cf322b-4e7f-44a5-96e7-c0cd62c01048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From Parquet in Blob Storage to Azure SQL using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b0935ea-7cca-4c4e-824a-c30adec63674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "%pip install pyodbc\n",
    "%pip install azure-storage-blob\n",
    "%pip install azure-data-tables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e5db48-0795-47cb-a5f4-6cc5b25c0b80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "419c6f44-81ce-4d60-b2ce-936893846dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.data.tables import TableServiceClient, TableClient\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import json\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "# Azure Blob Storage Config\n",
    "BLOB_CONNECTION_STRING = os.getenv(\"BLOB_CONNECTION_STRING\")\n",
    "BLOB_CONTAINER = \"weather-data\"\n",
    "BLOB_NAME = \"weather-data.parquet\"\n",
    "\n",
    "# Blob Container Config\n",
    "blob_service_client = BlobServiceClient.from_connection_string(BLOB_CONNECTION_STRING)\n",
    "container_client = blob_service_client.get_container_client(BLOB_CONTAINER)\n",
    "\n",
    "def read_parquet_from_blob():\n",
    "    global df\n",
    "\n",
    "    blob_client = container_client.get_blob_client(BLOB_NAME)\n",
    "    existing_data = io.BytesIO()\n",
    "    blob_client.download_blob().readinto(existing_data)\n",
    "    df = pd.read_parquet(existing_data)\n",
    "\n",
    "read_parquet_from_blob()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f394a5c-299e-4a2b-aa15-8808b1637f84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "import os\n",
    "\n",
    "# JDBC Config\n",
    "jdbc_hostname = \"weather-database.database.windows.net\"\n",
    "jdbc_database = \"weather-db\"\n",
    "jdbc_port = 1433\n",
    "\n",
    "# JDBC Credentials to access the database (Azure SQL) -> Check Connectors in the database to get the credentials\n",
    "jdbc_username = os.getenv(\"jdbc_username\")\n",
    "print(jdbc_username)\n",
    "jdbc_password = os.getenv(\"jdbc_password\")\n",
    "print(jdbc_password)\n",
    "driver_class = os.getenv(\"driver_class\")\n",
    "print(driver_class)\n",
    "\n",
    "jdbc_url = f\"jdbc:sqlserver://{jdbc_hostname}:{jdbc_port};database={jdbc_database};user={jdbc_username}@weather-database;password={jdbc_password};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n",
    "connection_properties = {\n",
    "    \"user\": jdbc_username,\n",
    "    \"password\": jdbc_password,\n",
    "    \"driver\": driver_class\n",
    "}\n",
    "\n",
    "# As default Databricks already has the JDBC driver for SQL Server. Just to check if it is realy installed\n",
    "try:\n",
    "    jvm = spark._jvm\n",
    "    driver = jvm.Class.forName(driver_class)\n",
    "    print(f\"✅ The JDBC driver {driver_class} is available!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ JDBC driver not found: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "536c4be6-5cf9-48f6-9cae-fa91f887fd67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Target Table Name\n",
    "table_name = \"dbo.WeatherTable\"\n",
    "\n",
    "#Parse the pandas dataframe to a Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# In my experience, to delete a table, it is better to navegate to database editor and delete it there (\"Query Editor (pre-visualization)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b3ff1cc-e440-4027-9abd-f472971c4b96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, TimestampType\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "# Function to processs the JSON data and return a tuple\n",
    "def parse_weather(row_json):\n",
    "    try:\n",
    "        data = json.loads(row_json)\n",
    "        return (\n",
    "            (\n",
    "                data.get(\"id\", None),  \n",
    "                data.get(\"name\", None),\n",
    "                data.get(\"main\", {}).get(\"temp\", 0),\n",
    "                data.get(\"main\", {}).get(\"feels_like\", 0),\n",
    "                data.get(\"main\", {}).get(\"humidity\", None),\n",
    "                data.get(\"main\", {}).get(\"pressure\", None),\n",
    "                data.get(\"wind\", {}).get(\"speed\", None),\n",
    "                data.get(\"wind\", {}).get(\"deg\", None),\n",
    "                data.get(\"weather\", [{}])[0].get(\"id\", None),\n",
    "                data.get(\"clouds\", {}).get(\"all\", None),\n",
    "                datetime.utcfromtimestamp(data.get(\"sys\", {}).get(\"sunrise\", 0)),\n",
    "                datetime.utcfromtimestamp(data.get(\"sys\", {}).get(\"sunset\", 0)),\n",
    "                datetime.utcfromtimestamp(data.get(\"dt\", 0)),\n",
    "                data.get(\"coord\", {}).get(\"lat\", None),\n",
    "                data.get(\"coord\", {}).get(\"lon\", None), \n",
    "                data.get(\"sys\", {}).get(\"country\", None)\n",
    "            )\n",
    "\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "# Dataframe Schema (Like the collumns of a database table)\n",
    "schema = StructType([\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"weather_description\", StringType(), True),\n",
    "    StructField(\"temperature\", FloatType(), True),\n",
    "    StructField(\"feels_like\", FloatType(), True),\n",
    "    StructField(\"humidity\", IntegerType(), True),\n",
    "    StructField(\"pressure\", IntegerType(), True),\n",
    "    StructField(\"wind_speed\", FloatType(), True),\n",
    "    StructField(\"wind_deg\", IntegerType(), True),\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"sunrise\", TimestampType(), True),\n",
    "    StructField(\"sunset\", TimestampType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"city_id\", IntegerType(), True),\n",
    "    StructField(\"city_name\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Iterate the lines of the created Spark DataFrame (spark_df)\n",
    "parsed_data = []\n",
    "for row in spark_df.collect():\n",
    "    parsed_row = parse_weather(row)\n",
    "    if parsed_row:\n",
    "        parsed_data.append(parsed_row)\n",
    "\n",
    "# Select and transform the data to a Spark DataFrame\n",
    "processed_df = spark_df.select(\n",
    "    col(\"coord.lat\").alias(\"latitude\"),\n",
    "    col(\"coord.lon\").alias(\"longitude\"),\n",
    "    col(\"weather\")[0][\"id\"].alias(\"id\"),\n",
    "    col(\"main.temp\").alias(\"temperature\"),\n",
    "    col(\"main.feels_like\").alias(\"feels_like\"),\n",
    "    col(\"main.humidity\").alias(\"humidity\"),\n",
    "    col(\"main.pressure\").alias(\"pressure\"),\n",
    "    col(\"wind.speed\").alias(\"wind_speed\"),\n",
    "    col(\"wind.deg\").alias(\"wind_deg\"),\n",
    "    col(\"clouds.all\").alias(\"cloud_coverage\"),\n",
    "    from_unixtime(col(\"sys.sunrise\")).alias(\"sunrise\"),\n",
    "    from_unixtime(col(\"sys.sunset\")).alias(\"sunset\"),\n",
    "    from_unixtime(col(\"dt\")).alias(\"timestamp\"),\n",
    "    col(\"id\").alias(\"city_id\"),\n",
    "    col(\"name\").alias(\"city_name\"),\n",
    "    col(\"sys.country\").alias(\"country\")\n",
    ")\n",
    "\n",
    "display(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef80ff0-d4c4-4344-84a0-81b9cc757d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Insert data from the created DataFrame 'processed_df' in the database using the JDBC connector\n",
    "try:\n",
    "    processed_df.write.jdbc(url=jdbc_url, table=table_name, mode=\"append\", properties=connection_properties)\n",
    "    print(\"✅ New data added with sucess!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in data insertion: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54832351-8b95-4d36-929c-18227fe55816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read data from the table in a Azure SQL\n",
    "try:\n",
    "    sql_query = f\"SELECT * FROM {table_name}\"\n",
    "    df_new = (spark.read\n",
    "      .format(\"jdbc\")\n",
    "      .option(\"url\", jdbc_url)\n",
    "      .option(\"dbtable\", f\"({sql_query}) as tmp\")\n",
    "      .option(\"user\", connection_properties[\"user\"])\n",
    "      .option(\"password\", connection_properties[\"password\"])\n",
    "      .load()\n",
    "     )\n",
    "    display(df_new)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Table not found: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc8e459f-45ef-4948-85d4-34a7b867e3ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Exercice example just to test Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c8cc75-1ba9-426f-8a3b-993c352d055b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data example\n",
    "data = [\n",
    "    (41.1496, -8.611, \"Clouds\", \"few clouds\", \"02d\", \"stations\", 289.88, 289.15, 289.88, 289.88, 1005, 59, \n",
    "     10000, 10.29, 160, 20, 1742393294, \"PT\", 1742366366, 1742409896, 0, \"Porto\", 200),\n",
    "    \n",
    "    (38.7169, -9.1399, \"Clear\", \"clear sky\", \"01n\", \"stations\", 285.32, 284.01, 283.0, 286.5, 1012, 72, \n",
    "     9000, 5.5, 90, 5, 1742393294, \"PT\", 1742366366, 1742409896, 0, \"Lisboa\", 200)\n",
    "]\n",
    "\n",
    "# Column example\n",
    "columns = [\n",
    "    \"coord_lat\", \"coord_lon\", \"weather_main\", \"weather_description\", \"weather_icon\", \"base\",\n",
    "    \"main_temp\", \"main_feels_like\", \"main_temp_min\", \"main_temp_max\",\n",
    "    \"main_pressure\", \"main_humidity\", \"visibility\", \n",
    "    \"wind_speed\", \"wind_deg\", \"clouds_all\", \"dt\",\n",
    "    \"sys_country\", \"sys_sunrise\", \"sys_sunset\", \"timezone\", \n",
    "    \"name\", \"cod\"\n",
    "]\n",
    "\n",
    "# Create a Spark DataFrame, combining both data and columns \n",
    "df_new = spark.createDataFrame(data, columns)\n",
    "\n",
    "#Notice: In recent studies, some articles say that using spark.sql is more efficient than spark.createDataFrame (To be evaluated)\n",
    "# Exibite the created DataFrame\n",
    "display(df_new)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "#3 - Read and process BlobStorge data and store it in Azure SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
